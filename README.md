# Umane Data Lake â€” Pipeline Monday â†’ AWS S3

## ğŸš€ VisÃ£o Geral

Este repositÃ³rio implementa um pipeline completo para ingestÃ£o, tratamento e organizaÃ§Ã£o de dados da plataforma **Monday.com** em um **Data Lake AWS** estruturado em trÃªs camadas:

- **Bronze** â†’ dados brutos (JSON)
- **Silver** â†’ dados normalizados e flatten (Parquet)
- **Gold** â†’ dataset analÃ­tico padronizado, pronto para BI e anÃ¡lises

Arquitetura:

```
Monday API â†’ Bronze (S3) â†’ Silver (S3) â†’ Gold (S3) â†’ Analytics
```

---

## ğŸ“‚ Estrutura do RepositÃ³rio

```
umane-datalake/
â”‚
â”œâ”€â”€ src/umane_datalake/
â”‚   â”œâ”€â”€ config.py               # Carrega variÃ¡veis de ambiente e constantes (buckets, tokens, etc.)
â”‚   â”œâ”€â”€ monday_client.py        # Cliente GraphQL da Monday (extraÃ§Ã£o de itens, colunas, paginaÃ§Ã£o)
â”‚   â”œâ”€â”€ s3_client.py            # FunÃ§Ãµes utilitÃ¡rias para upload/download no AWS S3
â”‚   â”œâ”€â”€ transformacao.py        # Bronze â†’ Silver (flatten, limpeza, parquet)
â”‚   â”œâ”€â”€ transformacao_ouro.py   # Silver â†’ Gold (padronizaÃ§Ã£o, curadoria, novas chaves)
â”‚   â”œâ”€â”€ pipeline.py             # Orquestrador principal do pipeline
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ§  Principais Funcionalidades

### ğŸ”¹ 1. ExtraÃ§Ã£o da API Monday (GraphQL)
- Leitura de boards e itens
- PaginaÃ§Ã£o automÃ¡tica
- Suporte a colunas simples e complexas (mirror, text, numbers, etc.)
- Salvamento dos dados brutos na camada Bronze (JSON)

### ğŸ”¹ 2. Bronze â†’ Silver
Executado via `transformacao.py`:
- Detecta automaticamente o formato do JSON
- Faz flatten das colunas
- Normaliza tipos
- Concatena mÃºltiplos arquivos
- Salva em formato **Parquet**, otimizado para anÃ¡lises

### ğŸ”¹ 3. Silver â†’ Gold
Executado via `transformacao_ouro.py`:
- PadronizaÃ§Ã£o de nomes de colunas
- CriaÃ§Ã£o de IDs independentes da plataforma Monday
- SomatÃ³rio e agregaÃ§Ãµes em campos numÃ©ricos
- Salvamento da camada ouro em S3

---

## ğŸ”§ Como Executar o Pipeline

### 1ï¸âƒ£ Criar e ativar um ambiente virtual (PowerShell)
```powershell
python -m venv venv
venv\Scripts\Activate.ps1
```

### 2ï¸âƒ£ Instalar dependÃªncias
```powershell
pip install -r requirements.txt
```

### 3ï¸âƒ£ Exportar variÃ¡veis de ambiente
```powershell
setx MONDAY_API_KEY "seu_token_aqui"
setx AWS_ACCESS_KEY_ID "xxxxx"
setx AWS_SECRET_ACCESS_KEY "xxxxx"
setx AWS_DEFAULT_REGION "xxxxx"
```

### 4ï¸âƒ£ Executar o pipeline
```powershell
python -m src.umane_datalake.pipeline
```

Isso irÃ¡:
1. Extrair dados da Monday
2. Criar arquivos Bronze â†’ Silver â†’ Gold automaticamente no S3

---

## â˜ ConfiguraÃ§Ã£o do S3 (Data Lake)

Os buckets esperados sÃ£o:

```
umane-datalake-bronze/
    monday/{board}/{YYYYMM}/{arquivo.json}

umane-datalake-prata/
    monday/{board}/{YYYYMM}/{arquivo.parquet}

umane-datalake-ouro/
    monday/{board}/{YYYYMM}/{arquivo_gold.parquet}
```

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.
