# Umane Data Lake ‚Äî Monday Pipeline ‚Äî Arquitetura Completa

## üöÄ Introdu√ß√£o

O projeto **Umane Data Lake** foi desenvolvido para criar um pipeline automatizado e escal√°vel, respons√°vel por coletar dados da plataforma **Monday.com**, organiz√°-los em camadas de um Data Lake (**Bronze, Silver e Gold**) e disponibiliz√°-los para an√°lises e uso estrat√©gico.

---

# üèõ Vis√£o Geral da Arquitetura

```
Monday API ‚Üí Bronze ‚Üí Silver ‚Üí Gold ‚Üí BI/Analytics
```

## üîç Diagrama Geral do Pipeline

```mermaid
flowchart TD

A[GitHub Actions - Workflow diario] --> B[run_pipeline]
B --> C[Extracao Monday API - GraphQL + paginacao]
C --> D[Camada Bronze - JSON no S3]
D --> E[Bronze para Silver - flatten + normalizacao]
E --> F[Camada Silver - Parquet no S3]
F --> G[Silver para Gold - curadoria]
G --> H[Camada Gold - Dataset analitico]
...
style A fill:#2e83ff,stroke:#1c4b99,color:white
style D fill:#ffcc66,stroke:#b8860b,color:#000
style F fill:#b3e6ff,stroke:#006b99,color:#000
style H fill:#00a86b,stroke:#006b43,color:#fff

---

# ‚öôÔ∏è Orquestra√ß√£o ‚Äî GitHub Actions

A pipeline executa diariamente √†s **06:00 (UTC‚àí3)** utilizando GitHub Actions:

```yaml
name: pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 9 * * *"

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.12"
      - run: pip install -r requirements.txt
      - name: Run pipeline
        env:
          MONDAY_API_TOKEN: ${{ secrets.MONDAY_API_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "us-east-1"
        run: |
          export PYTHONPATH="$PYTHONPATH:src"
          python -c "from umane_datalake.pipeline import run_pipeline; run_pipeline()"
```

---

# üß© M√≥dulos Internos

## monday_client.py
Respons√°vel por extrair dados da API Monday:

- autentica√ß√£o via token  
- pagina√ß√£o cursor-based  
- normaliza√ß√£o de colunas complexas  

```mermaid
sequenceDiagram
    participant A as GitHub Actions
    participant P as run_pipeline()
    participant M as Monday API
    participant S as S3 Bronze

    A->>P: Executar pipeline
    P->>M: Query GraphQL
    M-->>P: Items + cursor
    P->>S: Salvar JSON bruto
```

---

# ü•â Camada Bronze

Os dados crus s√£o salvos em JSON:

```
s3://umane-datalake-bronze/.../monday_raw_timestamp.json
```

---

# ü•à Camada Silver

Convers√£o Bronze ‚Üí Silver via `transformacao.py`:

- flatten de colunas  
- normaliza√ß√£o  
- transforma√ß√£o incremental  
- sa√≠da em Parquet  

```mermaid
flowchart TD
A[JSON Bronze] --> B[json_para_dataframe]
B --> C[process_item]
C --> D[concat]
D --> E[Salvar Parquet Silver]
```

---

# ü•á Camada Gold

Curadoria final via `transformacao_ouro.py`:

- normaliza√ß√£o snake_case  
- uuid5 determin√≠stico  
- somat√≥ria de colunas compostas  
- dataset pronto para BI  

```mermaid
flowchart TD
A[Silver Parquet] --> B[normaliza√ß√£o]
B --> C[uuid5]
C --> D[tratamento num√©rico]
D --> E[Gold Parquet]
```

---

# üéØ Pipeline Principal ‚Äî run_pipeline()

```mermaid
graph LR
P[run_pipeline] --> M[busca_dados_monday]
P --> S1[transformar Bronze->Silver]
S1 --> S2[monday_items.parquet]
P --> G[criar Gold]
G --> O[gold.parquet]
```

---

# ‚ñ∂ Execu√ß√£o Local

```bash
pip install -r requirements.txt
export PYTHONPATH="$PYTHONPATH:src"
export MONDAY_API_TOKEN="..."
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
python -c "from umane_datalake.pipeline import run_pipeline; run_pipeline()"
```

---

# üîê Secrets Necess√°rios

| Nome | Descri√ß√£o |
|------|-----------|
| MONDAY_API_TOKEN | Token da API Monday |
| AWS_ACCESS_KEY_ID | Access key AWS |
| AWS_SECRET_ACCESS_KEY | Secret key AWS |
| AWS_DEFAULT_REGION | Regi√£o AWS |

---

## üìú Licen√ßa

Defina aqui a licen√ßa do projeto (ex.: MIT, Apache 2.0).
