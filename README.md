# Umane Data Lake â€” Monday Pipeline â€” Arquitetura Completa

## ğŸš€ IntroduÃ§Ã£o

O projeto **Umane Data Lake** foi desenvolvido para criar um pipeline robusto, automatizado e escalÃ¡vel, responsÃ¡vel por coletar dados da plataforma **Monday.com**, organizÃ¡-los em camadas de um Data Lake moderno (**Bronze, Silver e Gold**) e disponibilizÃ¡-los para anÃ¡lises e uso estratÃ©gico.

---

# ğŸ› VisÃ£o Geral da Arquitetura

```
Monday API â†’ Bronze â†’ Silver â†’ Gold â†’ BI/Analytics
```

## ğŸ” Diagrama Geral do Pipeline

```mermaid
flowchart TD

A[GitHub Actions<br/>Workflow diÃ¡rio] --> B[run_pipeline()]
B --> C[ExtraÃ§Ã£o Monday API<br/>GraphQL + paginaÃ§Ã£o]
C --> D[Salvar Bronze<br/>JSON no S3]
D --> E[TransformaÃ§Ã£o Bronze â†’ Silver<br/>flatten + normalizaÃ§Ã£o]
E --> F[Camada Silver<br/>Parquet no S3]
F --> G[TransformaÃ§Ã£o Silver â†’ Gold<br/>curadoria]
G --> H[Camada Gold<br/>Dataset analÃ­tico]

style A fill:#2e83ff,stroke:#1c4b99,color:white
style D fill:#ffcc66,stroke:#b8860b,color:#000
style F fill:#b3e6ff,stroke:#006b99,color:#000
style H fill:#00a86b,stroke:#006b43,color:#fff
```

---

# âš™ï¸ OrquestraÃ§Ã£o â€” GitHub Actions

A pipeline executa diariamente Ã s **06:00 (UTCâˆ’3)** utilizando GitHub Actions:

```yaml
name: pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 9 * * *"

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.12"
      - run: pip install -r requirements.txt
      - name: Run pipeline
        env:
          MONDAY_API_TOKEN: ${{ secrets.MONDAY_API_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "us-east-1"
        run: |
          export PYTHONPATH="$PYTHONPATH:src"
          python -c "from umane_datalake.pipeline import run_pipeline; run_pipeline()"
```

---

# ğŸ§© MÃ³dulos Internos

## monday_client.py
ResponsÃ¡vel por extrair dados da API Monday:

- autenticaÃ§Ã£o via token  
- paginaÃ§Ã£o cursor-based  
- normalizaÃ§Ã£o de colunas complexas  

```mermaid
sequenceDiagram
    participant A as GitHub Actions
    participant P as run_pipeline()
    participant M as Monday API
    participant S as S3 Bronze

    A->>P: Executar pipeline
    P->>M: Query GraphQL
    M-->>P: Items + cursor
    P->>S: Salvar JSON bruto
```

---

# ğŸ¥‰ Camada Bronze

Os dados crus sÃ£o salvos em JSON:

```
s3://umane-datalake-bronze/.../monday_raw_timestamp.json
```

---

# ğŸ¥ˆ Camada Silver

ConversÃ£o Bronze â†’ Silver via `transformacao.py`:

- flatten de colunas  
- normalizaÃ§Ã£o  
- transformaÃ§Ã£o incremental  
- saÃ­da em Parquet  

```mermaid
flowchart TD
A[JSON Bronze] --> B[json_para_dataframe]
B --> C[process_item]
C --> D[concat]
D --> E[Salvar Parquet Silver]
```

---

# ğŸ¥‡ Camada Gold

Curadoria final via `transformacao_ouro.py`:

- normalizaÃ§Ã£o snake_case  
- uuid5 determinÃ­stico  
- somatÃ³ria de colunas compostas  
- dataset pronto para BI  

```mermaid
flowchart TD
A[Silver Parquet] --> B[normalizaÃ§Ã£o]
B --> C[uuid5]
C --> D[tratamento numÃ©rico]
D --> E[Gold Parquet]
```

---

# ğŸ¯ Pipeline Principal â€” run_pipeline()

```mermaid
graph LR
P[run_pipeline] --> M[busca_dados_monday]
P --> S1[transformar Bronze->Silver]
S1 --> S2[monday_items.parquet]
P --> G[criar Gold]
G --> O[gold.parquet]
```

---

# â–¶ ExecuÃ§Ã£o Local

```bash
pip install -r requirements.txt
export PYTHONPATH="$PYTHONPATH:src"
export MONDAY_API_TOKEN="..."
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
python -c "from umane_datalake.pipeline import run_pipeline; run_pipeline()"
```

---

# ğŸ” Secrets NecessÃ¡rios

| Nome | DescriÃ§Ã£o |
|------|-----------|
| MONDAY_API_TOKEN | Token da API Monday |
| AWS_ACCESS_KEY_ID | Access key AWS |
| AWS_SECRET_ACCESS_KEY | Secret key AWS |
| AWS_DEFAULT_REGION | RegiÃ£o AWS |

---

## ğŸ“œ LicenÃ§a

Defina aqui a licenÃ§a do projeto (ex.: MIT, Apache 2.0).
